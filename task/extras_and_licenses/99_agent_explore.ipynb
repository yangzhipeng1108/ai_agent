{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55ef82e5",
   "metadata": {},
   "source": [
    "<center><a href=\"https://www.nvidia.cn/training/\"><img src=\"https://dli-lms.s3.amazonaws.com/assets/general/DLI_Header_White.png\" width=\"400\" height=\"186\" /></a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63c4a3e",
   "metadata": {},
   "source": [
    "# <font color=\"#76b900\"> **补充 Notebook:** Agent Executer 代码块 </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528095d4-14fb-44a7-9484-e0c964168312",
   "metadata": {},
   "source": [
    "**本 notebook 可帮助您在不通过源代码简单了解 Agent Executor loop 的工作原理** 以下内容可能不是最新的，但足够简明清晰。 \n",
    "\n",
    "打开 [AgentExecutor call method](https://github.com/langchain-ai/langchain/blob/c2d1d903fa35b91018b4d777db2b008fcbaa9fbc/langchain/agents/agent.py#L378)，您会发现事件循环有这样的结构：\n",
    "\n",
    "```python\n",
    "class AgentExecutor(Chain, BaseModel):\n",
    "    def _call(self, inputs: Dict[str, str]) -> Dict[str, Any]:\n",
    "\n",
    "        # Do any preparation necessary when receiving a new input.\n",
    "        self.agent.prepare_for_new_call()\n",
    "\n",
    "        ## Setup the event loop\n",
    "        for iterations in range(self.max_iterations):\n",
    "            # Call the LLM to see what to do.\n",
    "            output = self.agent.plan(intermediate_steps, **inputs)\n",
    "\n",
    "            # If the agent produced an AgentFinish output, finish up.\n",
    "            if isinstance(output, AgentFinish):\n",
    "                return self._return(output, intermediate_steps)\n",
    "\n",
    "            # Otherwise we lookup and try to use the tool\n",
    "            if output.tool in name_to_tool_map:\n",
    "                tool = name_to_tool_map[output.tool]\n",
    "                observation = tool.func(output.tool_input) ## Get observation from the tool\n",
    "                return_direct = tool.return_direct         ## Should we break out of the loop?\n",
    "            else:\n",
    "                observation = f\"{output.tool} is not a valid tool, try another one.\"\n",
    "\n",
    "            ## Record the recent interaction in the intermediate_steps\n",
    "            intermediate_steps.append((output, observation))\n",
    "\n",
    "            ## If our tool produced a final output, finish up.\n",
    "            if return_direct:\n",
    "                output = AgentFinish({self.agent.return_values[0]: observation}, \"\")\n",
    "                return self._return(output, intermediate_steps)\n",
    "\n",
    "        ## If a good output is never found, just give up and perform default exit\n",
    "        output = self.agent.return_stopped_response(\n",
    "            self.early_stopping_method, intermediate_steps, **inputs\n",
    "        )\n",
    "        return self._return(output, intermediate_steps)\n",
    "\n",
    "```\n",
    "\n",
    "[ZeroShotAgent](https://github.com/langchain-ai/langchain/blob/c2d1d903fa35b91018b4d777db2b008fcbaa9fbc/langchain/agents/mrkl/base.py#L51) 的逻辑如下：\n",
    "\n",
    "```python\n",
    "class ZeroShotAgent(Agent):\n",
    "    \n",
    "    ##############################################################################\n",
    "    ## From Agent superclass\n",
    "    llm_chain: LLMChain\n",
    "    allowed_tools: Optional[List[str]] = None\n",
    "    return_values: List[str] = [\"output\"]\n",
    "    finish_tool_name: str = \"Final Answer\"\n",
    "    \n",
    "    def plan(\n",
    "        self, intermediate_steps: List[Tuple[AgentAction, str]], **kwargs: Any\n",
    "    ) -> Union[AgentAction, AgentFinish]:\n",
    "        \"\"\"Given input, decided what to do\"\"\"\n",
    "        full_inputs = self.get_full_inputs(intermediate_steps, **kwargs)\n",
    "        action = self._get_next_action(full_inputs)\n",
    "        if action.tool == self.finish_tool_name:\n",
    "            return AgentFinish({\"output\": action.tool_input}, action.log)\n",
    "        return action\n",
    "    \n",
    "    def _get_next_action(self, full_inputs: Dict[str, str]) -> AgentAction:\n",
    "        \"\"\"Given the full input to the LLM, make the next prediction\"\"\"\n",
    "        full_output = self.llm_chain.predict(**full_inputs)\n",
    "        action, action_input = self._extract_tool_and_input(full_output)\n",
    "        return AgentAction(\n",
    "            tool=action, tool_input=action_input, log=full_output\n",
    "        )\n",
    "\n",
    "    ##############################################################################\n",
    "    ## From ZeroShotAgent specifically\n",
    "    def _extract_tool_and_input(self, text: str) -> Optional[Tuple[str, str]]:\n",
    "        \"\"\"\n",
    "        Parse out the action and input from the LLM output.\n",
    "        The string starting with \"Action:\" and the following string starting\n",
    "        with \"Action Input:\" should be separated by a newline.\n",
    "        \"\"\"\n",
    "        if \"Final Answer:\" in llm_output:\n",
    "            return \"Final Answer\", llm_output.split(\"Final Answer:\")[-1].strip()\n",
    "        action, action_input = format_match(r\"Action: (.*?)\\nAction Input: (.*)\", llm_output)\n",
    "        if not action or not action_input: \n",
    "            raise ValueError(f\"Could not parse LLM output: `{llm_output}`\")\n",
    "        return action, action_input\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fcd6111",
   "metadata": {},
   "source": [
    "## 评估的初始代码"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3705839e",
   "metadata": {},
   "source": [
    "```python\n",
    "####################################################################################\n",
    "## TODO: Your workspace is below\n",
    "    \n",
    "llama_full_prompt = PromptTemplate.from_template(\n",
    "    template=\"<s>[INST]<<SYS>>{sys_msg}<</SYS>>\\n\\nContext:\\n{history}\\n\\nHuman: {input}\\n[/INST] {primer}\",\n",
    ")\n",
    "\n",
    "llama_prompt = llama_full_prompt.partial(\n",
    "    sys_msg = ( \n",
    "        \"You are a helpful, respectful and honest AI assistant.\"\n",
    "        \"\\nAlways answer as helpfully as possible, while being safe.\"\n",
    "        \"\\nPlease be brief and efficient unless asked to elaborate, and follow the conversation flow.\"\n",
    "        \"\\nYour answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content.\"\n",
    "        \"\\nEnsure that your responses are socially unbiased and positive in nature.\"\n",
    "        \"\\nIf a question does not make sense or is not factually coherent, explain why instead of answering something incorrect.\" \n",
    "        \"\\nIf you don't know the answer to a question, please don't share false information.\"\n",
    "        \"\\nIf the user asks for a format to output, please follow it as closely as possible.\"\n",
    "    ),\n",
    "    primer = \"\",\n",
    "    history = \"\",\n",
    ")\n",
    "\n",
    "####################################################################################\n",
    "## THESE MIGHT BE USEFUL IMPORTS!\n",
    "\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "img_pipe = pipeline(\"image-to-text\", model=\"Salesforce/blip-image-captioning-large\")\n",
    "emo_pipe = pipeline('sentiment-analysis', 'SamLowe/roberta-base-go_emotions')  \n",
    "zsc_pipe = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "tox_pipe = pipeline(\"text-classification\", model=\"nicholasKluge/ToxicityModel\")\n",
    "## WARNING: toxic_pipe returns the reward, where reward = 1 - toxicity\n",
    "\n",
    "###################################################################################\n",
    "\n",
    "\n",
    "class MyAgent(MyAgentBase):\n",
    "    \n",
    "    ## Instance methods that can be passed in as BaseModel arguments. \n",
    "    ## Will be associated with self\n",
    "    \n",
    "    general_prompt : PromptTemplate\n",
    "    llm            : BaseLLM\n",
    "    \n",
    "    general_chain  : Optional[LLMChain]\n",
    "    max_messages   : int                   = Field(10, gt=1)\n",
    "    \n",
    "    temperature    : float                 = Field(0.6, gt=0, le=1)\n",
    "    max_new_tokens : int                   = Field(128, ge=1, le=2048)\n",
    "    eos_token_id   : Union[int, List[int]] = Field(2, ge=0)\n",
    "    gen_kw_keys = ['temperature', 'max_new_tokens', 'eos_token_id']\n",
    "    gen_kw = {}\n",
    "    \n",
    "    user_toxicity  : float = 0.5\n",
    "    user_emotion   : str = \"Unknown\"\n",
    "    \n",
    "    \n",
    "    @root_validator\n",
    "    def validate_input(cls, values: Any) -> Any:\n",
    "        '''Think of this like the BaseModel's __init__ method'''\n",
    "        if not values.get('general_chain'):\n",
    "            llm = values.get('llm')\n",
    "            prompt = values.get(\"general_prompt\")\n",
    "            values['general_chain'] = LLMChain(llm=llm, prompt=prompt)  ## <- Feature stop \n",
    "        values['gen_kw'] = {k:v for k,v in values.items() if k in values.get('gen_kw_keys')}\n",
    "        return values\n",
    "    \n",
    "\n",
    "    def plan(self, intermediate_steps: List[Tuple[AgentAction, str]], **kwargs: Any): \n",
    "        '''Takes in previous logic and generates the next action to take!'''\n",
    "        \n",
    "        ## [Base Case] Default message to start off the loop. DO NOT OVERRIDE\n",
    "        tool, response = \"Ask-For-Input Tool\", \"Hello World! How can I help you?\"\n",
    "        if len(intermediate_steps) == 0:\n",
    "            return self.action(tool, response)\n",
    "        \n",
    "        ## History of past agent queries/observations\n",
    "        queries      = [step[0].tool_input for step in intermediate_steps]\n",
    "        observations = [step[1]            for step in intermediate_steps]\n",
    "        last_obs     = observations[-1]    # Most recent observation (i.e. user input)\n",
    "\n",
    "        #############################################################################\n",
    "        ## FOR THIS METHOD, ONLY MODIFY THE ENCLOSED REGION\n",
    "        \n",
    "        ## [!] Probably a good spot for your user statistics tracking\n",
    "        \n",
    "        ## [Stop Case] If the conversation is getting too long, wrap it up\n",
    "        if len(observations) >= self.max_messages:\n",
    "            response = \"Thanks so much for the chat, and hope to see ya later! Goodbye!\"\n",
    "            return self.action(tool, response, finish=True)\n",
    "        \n",
    "        ## [!] Probably a good spot for your input-augmentation steps\n",
    "\n",
    "        ## [Default Case] If observation is provided and you want to respond... do it!\n",
    "        with SetParams(llm, **self.gen_kw):\n",
    "            response = self.general_chain.run(last_obs)\n",
    "            \n",
    "        ## [!] Probably a good spot for your output-postprocessing steps\n",
    "        \n",
    "        ## FOR THIS METHOD, ONLY MODIFY THE ENCLOSED REGION\n",
    "        #############################################################################\n",
    "        \n",
    "        ## [Default Case] Send over the response back to the user and get their input!\n",
    "        return self.action(tool, response)\n",
    "    \n",
    "\n",
    "    def reset(self):\n",
    "        self.user_toxicity = 0\n",
    "        self.user_emotion = \"Unknown\"\n",
    "        if getattr(self.general_chain, 'memory', None) is not None:\n",
    "            self.general_chain.memory.clear()  ## Hint about what general_chain should be...\n",
    "\n",
    "\n",
    "####################################################################################\n",
    "## Define how you want your conversation to go. You can also use your own input\n",
    "## The below example in conversation_gen exercises some of the requirements.\n",
    "\n",
    "student_name = \"John Doe\"   ## TODO: What's your name\n",
    "ask_via_input = False       ## TODO: When you're happy, try supplying your own inputs\n",
    "\n",
    "def conversation_gen():\n",
    "    yield f\"Hello! How's it going? My name is {student_name}! Nice to meet you!\"\n",
    "    yield \"Please tell me a little about deep learning!\"\n",
    "    yield \"What's my name?\"                                  ## Memory buffer\n",
    "    yield \"I'm not feeling very good -_-. What should I do\"  ## Emotion sensor\n",
    "    yield \"No, I'm done talking! Thanks so much!\"            ## Conversation ender\n",
    "    yield \"Goodbye!\"                                         ## Conversation ender x2\n",
    "    raise KeyboardInterrupt()\n",
    "\n",
    "conversation_instance = conversation_gen()\n",
    "converser = lambda x: next(conversation_instance)\n",
    "\n",
    "if ask_via_input:\n",
    "    converser = input  ## Alternatively, supply your own inputs\n",
    "\n",
    "agent_kw = dict(\n",
    "    llm = llm,\n",
    "    general_prompt = llama_prompt,\n",
    "    max_new_tokens = 128,\n",
    "    eos_token_id = [2]   \n",
    ")\n",
    "\n",
    "agent_ex = AgentExecutor.from_agent_and_tools(\n",
    "    agent = MyAgent(**agent_kw),\n",
    "    tools=[AskForInputTool(converser).get_tool()], \n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "## NOTE: You might want to comment this out to make testing the autograder easier\n",
    "try: agent_ex.run(\"\")\n",
    "except KeyboardInterrupt: print(\"KeyboardInterrupt\")\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
